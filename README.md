
# ğŸ§  RAG QA Chatbot Application

A Retrieval-Augmented Generation (RAG) chatbot that allows users to upload documents (PDF or DOCX), ask questions in natural language, and receive accurate answers based on the content of the uploaded documents â€” all powered by a vector database and LLMs.

---

## ğŸš€ Features

- âœ… Upload multiple documents (PDF, DOCX)
- âœ… Text chunking using LangChain's `RecursiveCharacterTextSplitter`
- âœ… Embedding with `sentence-transformers`
- âœ… Vector storage using FAISS with incremental updates
- âœ… Question-answering with Together.ai API (e.g., Mistral-7B-Instruct)
- âœ… Caching of previous questions to speed up repeated queries
- âœ… Persistent FAISS index and chunk storage between sessions
- âœ… Clean and simple Streamlit UI
- âœ… Fully containerized with Docker

---

## ğŸ“¦ Installation (Local Dev)

```bash
# Clone the repository
git clone https://github.com/MehmetAltinkurt/rag-chatbot.git
cd rag-chatbot

# Set up Python environment
pip install -r requirements.txt

# Create a .env file
echo "TOGETHER_API_KEY=your_api_key_here" > .env

# Run the app
streamlit run rag_chatbot/ui.py
```

---

## ğŸ³ Run from DockerHub

```bash
docker run -p 8501:8501 mehmetaltinkurt/rag-chatbot:v1
```
---

## ğŸ”‘ Environment Variables

| Variable          | Description                  |
|------------------|------------------------------|
| `TOGETHER_API_KEY` | Your Together.ai API key     |

---

## ğŸ§ª How It Works

1. Upload documents (PDF or DOCX)
2. Each document is parsed and split into overlapping chunks
3. Chunks are embedded and added to the FAISS index
4. When a question is asked:
   - Top-10 relevant chunks are retrieved
   - A prompt is built and sent to the LLM (Together.ai)
   - The answer is generated and returned

---

## ğŸ“ Project Structure

```
rag_chatbot/
â”œâ”€â”€ ui.py                  # Streamlit UI
â”œâ”€â”€ file_handler.py        # PDF/DOCX reading
â”œâ”€â”€ text_splitter.py       # LangChain chunking
â”œâ”€â”€ vector_store.py        # FAISS with persistent chunks
â”œâ”€â”€ retriever.py           # Retrieval logic
â”œâ”€â”€ llm_api.py             # Together.ai interaction
â”œâ”€â”€ rag_pipeline.py        # Orchestrates RAG pipeline
data/                      # Stored FAISS index + chunks
Dockerfile
requirements.txt
.env
```

---

## âœ… Evaluation Highlights

- ğŸ” **Relevant Document Retrieval**: LangChain splitter + FAISS for fast retrieval
- ğŸ§  **LLM Output**: Controlled prompt format + stop sequences
- ğŸ§ª **Tested**: Dockerized and tested with 100+ page documents

---

## ğŸ“¬ Submission Info

This project was developed as part of a technical interview task. For more information, see the original task.

---

## ğŸ“œ License

MIT License

