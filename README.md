
# ğŸ§  RAG QA Chatbot Application

A Retrieval-Augmented Generation (RAG) chatbot that allows users to upload documents (PDF or DOCX), ask questions in natural language, and receive accurate answers based on the content of the uploaded documents â€” all powered by a vector database and local or API-based LLMs.

---

## ğŸš€ Features

- âœ… Upload multiple documents (PDF, DOCX)
- âœ… Text chunking using LangChain's `RecursiveCharacterTextSplitter`
- âœ… Embedding with `sentence-transformers`
- âœ… Vector storage using FAISS with persistent caching
- âœ… Question-answering using:
  - ğŸ”Œ Together.ai API
  - ğŸ’» Local HuggingFace LLMs (with CPU/GPU support)
- âœ… Select LLM and model at runtime via sidebar
- âœ… Adjustable top-k chunk retrieval slider
- âœ… Cached answers for faster repeated queries
- âœ… Reset App button to clear everything
- âœ… Chunk-source tracking and grouped display by document
- âœ… Works offline or online
- âœ… Fully containerized with Docker

---

## ğŸ§‘â€ğŸ’» How to Run

### ğŸ”¹ 1. Clone the repository (optional for local dev)

```bash
git clone https://github.com/MehmetAltinkurt/rag-chatbot.git
cd rag-chatbot
```

---

### ğŸ”¹ 2. Run with Docker (recommended)

You can pull and run the prebuilt image directly from Docker Hub:

```bash
docker pull mehmetaltinkurt/rag-chatbot:v2
docker run -p 8501:8501 --env-file .env mehmetaltinkurt/rag-chatbot:v2
```

To enable GPU support (if using `nvidia-docker`):

```bash
docker run --gpus all -p 8501:8501 --env-file .env mehmetaltinkurt/rag-chatbot:v2
```

---

## ğŸ§ª Local Development

### ğŸ”¹ Install dependencies

```bash
pip install -r requirements.txt
```

Or with CUDA:

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

---

### ğŸ”¹ Set up `.env`

Create a `.env` file in your project root:

```env
TOGETHER_API_KEY=your_together_api_key
HUGGINGFACE_HUB_TOKEN=your_huggingface_token
```

---

### ğŸ”¹ Run the app locally

```bash
streamlit run rag_chatbot/ui.py
```

---

## ğŸ¤– Run in Google Colab

1. Mount secrets securely:

```python
from google.colab import userdata
import os

os.environ["TOGETHER_API_KEY"] = userdata.get("TOGETHER_API_KEY")
os.environ["HUGGINGFACE_HUB_TOKEN"] = userdata.get("HUGGINGFACE_HUB_TOKEN")
```

2. Launch the app:

```python
!git clone https://github.com/MehmetAltinkurt/rag-chatbot.git
%cd rag-chatbot
!pip install -r requirements.txt
!streamlit run rag_chatbot/ui.py &
from pyngrok import ngrok
print(ngrok.connect(8501))
```

---

## ğŸ§ª Usage Tips

- Use the sidebar to:
  - Choose LLM mode (Together.ai or Local)
  - Set the model name
  - Adjust Top-K chunks retrieved
- Uploaded files and answers are cached
- ğŸ’¬ Answer and ğŸ“„ Source Chunks are shown side-by-side
- Click **Reset App** to clear files, chunks, and history

---

## ğŸ“ Project Structure

```
rag_chatbot/
â”œâ”€â”€ ui.py                  # Streamlit UI logic
â”œâ”€â”€ file_handler.py        # PDF/DOCX parsing
â”œâ”€â”€ text_splitter.py       # Chunking logic
â”œâ”€â”€ vector_store.py        # FAISS logic
â”œâ”€â”€ retriever.py           # Retriever wrapper
â”œâ”€â”€ retriever_manager.py   # Session-safe retriever state
â”œâ”€â”€ llm_api.py             # Together.ai completion
â”œâ”€â”€ local_llm.py           # Transformers-based local LLM
â”œâ”€â”€ llm_manager.py         # LLM mode switcher
data/                      # Stores chunks and FAISS index
```

---

## ğŸ“¦ Docker Image

- ğŸ³ DockerHub: [`mehmetaltinkurt/rag-chatbot:v2`](https://hub.docker.com/r/mehmetaltinkurt/rag-chatbot)
- Includes full environment and runs offline or with Together.ai

---

## ğŸ§¾ License

MIT License
